{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da77599",
   "metadata": {},
   "source": [
    "# Exercises week 38\n",
    "\n",
    "## September 15-19\n",
    "\n",
    "## Resampling and the Bias-Variance Trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f27b0e",
   "metadata": {},
   "source": [
    "### Learning goals\n",
    "\n",
    "After completing these exercises, you will know how to\n",
    "\n",
    "- Derive expectation and variances values related to linear regression\n",
    "- Compute expectation and variances values related to linear regression\n",
    "- Compute and evaluate the trade-off between bias and variance of a model\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Complete the following exercises while working in a jupyter notebook. Then, in canvas, include\n",
    "\n",
    "- The jupyter notebook with the exercises completed\n",
    "- An exported PDF of the notebook (https://code.visualstudio.com/docs/datascience/jupyter-notebooks#_export-your-jupyter-notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af8e3",
   "metadata": {},
   "source": [
    "## Use the books!\n",
    "\n",
    "This week deals with various mean values and variances in linear regression methods (here it may be useful to look up chapter 3, equation (3.8) of [Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer](https://www.springer.com/gp/book/9780387848570)).\n",
    "\n",
    "For more discussions on Ridge regression and calculation of expectation values, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "\n",
    "The exercises this week are also a part of project 1 and can be reused in the theory part of the project.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "We assume that there exists a continuous function $f(\\boldsymbol{x})$ and a normal distributed error $\\boldsymbol{\\varepsilon}\\sim N(0, \\sigma^2)$ which describes our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7d0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = f(\\boldsymbol{x})+\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf981a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We further assume that this continous function can be modeled with a linear model $\\mathbf{\\tilde{y}}$ of some features $\\mathbf{X}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4189366",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{\\tilde{y}} + \\boldsymbol{\\varepsilon} = \\boldsymbol{X}\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3dffc",
   "metadata": {},
   "source": [
    "**For clarity**, the above should maybe have been written in this way, with $\\boldsymbol{e}$ as the residual, $\\boldsymbol{e} = \\boldsymbol{y} - \\boldsymbol{\\tilde{y}}$, so not to confuse it with the noise in $\\boldsymbol{y}$: \n",
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{\\tilde{y}} + \\boldsymbol{e} = \\boldsymbol{X}\\boldsymbol{\\beta} +\\boldsymbol{e}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fca21b",
   "metadata": {},
   "source": [
    "We therefore get that our data $\\boldsymbol{y}$ has an expectation value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$, that is $\\boldsymbol{y}$ follows a normal distribution with mean value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0c7e6",
   "metadata": {},
   "source": [
    "## Exercise 1: Expectation values for ordinary least squares expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878c699",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7007d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\boldsymbol{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d6207",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> We remember that $\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$, so:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\mathbb{E} \\left[ \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y} \\right]\n",
    "$$\n",
    "\n",
    "Since the feature matrix $\\boldsymbol{X}$ is non-stochastic we can treat everything to the left of $\\boldsymbol{y}$ as a constant, and apply the linearity of the expectation value operator like this, $ \\mathbb{E}[cA] = \\mathbb{E}[c] \\mathbb{E}[A] = c \\mathbb{E}[A]$. The expectation value of a constant or non-stochastic variable is it self. We then have:\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\mathbb{E} \\left[ \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\right] \\mathbb{E} \\left[  \\boldsymbol{y} \\right] = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\mathbb{E} \\left[  \\boldsymbol{y} \\right]\n",
    "$$\n",
    "\n",
    "For the last term we have $\\boldsymbol{y} = f(\\boldsymbol{x}) + \\boldsymbol{\\varepsilon} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$:\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\boldsymbol{y} \\right] = \\mathbb{E} \\left[ f(\\boldsymbol{x}) + \\boldsymbol{\\varepsilon} \\right] = \\mathbb{E} \\left[ f(\\boldsymbol{x}) \\right] + \\mathbb{E} \\left[ \\boldsymbol{\\varepsilon} \\right]\n",
    "$$\n",
    "\n",
    "The error term $\\boldsymbol{\\varepsilon}$ follows the normal distribution, with expectation value equal zero, so we are left with: $\\mathbb{E} \\left[ \\boldsymbol{y} \\right] = \\mathbb{E} \\left[ f(\\boldsymbol{x}) \\right] = \\mathbb{E} \\left[ \\boldsymbol{X \\beta} \\right] = \\boldsymbol{X \\beta}$.\n",
    "\n",
    "Returning to the main expression, we have: \n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{X \\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "Which we were to show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e93394",
   "metadata": {},
   "source": [
    "**b)** Show that the variance of $\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}$ is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b65be",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\sigma^2 \\, (\\mathbf{X}^{T} \\mathbf{X})^{-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2143684",
   "metadata": {},
   "source": [
    "We can use the last expression when we define a [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) for the parameters $\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}$.\n",
    "A given parameter ${\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}}_j$ is given by the diagonal matrix element of the above matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bd2ad",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> For the variance of a stochastic variable $X$ we have that:\n",
    "$$\n",
    "\\mathbf{Var}(X) = \\mathbb{E} \\left[ X^2 \\right] - \\mathbb{E} \\left[ X \\right]^2\n",
    "$$\n",
    "\n",
    "We write:\n",
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}) = \\mathbb{E} \\left[ \\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}^2 \\right] - \\mathbb{E} \\left[ \\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}} \\right]^2\n",
    "$$\n",
    "\n",
    "Using this expression we would have to take into account that we are dealing with vectors and matrices, and carefully construct it so to represent the squared terms correctly. It would also be quite some algebra to be done. Therefore we try to use some of the computational rules of the variance instead.\n",
    "So we again start with the closed form:\n",
    "$$\n",
    "\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "Substituting, $y = \\boldsymbol{X \\beta} + \\boldsymbol{\\varepsilon}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}} = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\left( \\boldsymbol{X \\beta} + \\boldsymbol{\\varepsilon} \\right) \n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{X \\beta} + \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T  \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\boldsymbol{\\beta} + \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T  \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "\n",
    "Since $\\beta$ is the true coefficient vector, it must have zero variance. we can also think of it as a vector that ads a constant value for each element in the vector representing the second term. Therefore this term is to be considered a constant. For vectors and matrices we have the following rule for the variance of a constant and a product: $\\mathbf{Var}(\\boldsymbol{a} + \\boldsymbol{AM}) = \\boldsymbol{A} \\, \\mathbf{Var}(\\boldsymbol{M}) \\, \\boldsymbol{A}^T$, where $a$ is a non stochastic vector, $\\boldsymbol{A}$ a non stochastic matrix, and $\\boldsymbol{M}$ a vector with stochastic elements. Inserting directly from the expression above we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\, \\mathbf{Var}(\\boldsymbol{\\varepsilon}) \\, \\left( \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\right)^T\n",
    "$$\n",
    "\n",
    "For $\\boldsymbol{\\varepsilon}$, which elements follows the normal distribution, we have from the definition $\\boldsymbol{\\sigma}^2$, or it could be written $\\sigma^2 \\, \\boldsymbol{I}$. Now, taking the transpose above, and substituting in for the variance:\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\, \\sigma^2 \\, \\boldsymbol{I} \\, \\boldsymbol{X} \\left( \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "Rearranging and combining factors:\n",
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\sigma^2 \\, \\left( \\left( \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "Further reading: Gram matrix.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2dc22",
   "metadata": {},
   "source": [
    "## Exercise 2: Expectation values for Ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893e3e7",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{Ridge}}$ show that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc571f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big]=(\\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I}_{pp})^{-1} (\\mathbf{X}^{\\top} \\mathbf{X})\\boldsymbol{\\beta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028209a1",
   "metadata": {},
   "source": [
    "We see that $\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big] \\not= \\mathbb{E} \\big[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\big ]$ for any $\\lambda > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffaee7",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> We remember that:\n",
    "$$ \\boldsymbol{\\hat{\\beta}_{\\text{Ridge}}} = \\left( \\boldsymbol{X}^T\\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y} $$\n",
    "\n",
    "\n",
    "From the derivation of $\\mathbb{E}(\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}})$ above, we just smuggle in the $\\lambda\\boldsymbol{I}$ term since it is to be considered a constant, and thus won't affect the calculation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}_{\\text{Ridge}}}) = \\mathbb{E} \\left[ \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1} \\boldsymbol{X}^T \\right] \\mathbb{E} \\left[  \\boldsymbol{y} \\right] = \\left( \\boldsymbol{X}^T \\boldsymbol{X} +\\lambda\\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\mathbb{E} \\left[  \\boldsymbol{y} \\right]\n",
    "$$\n",
    "\n",
    "And we have seen that $\\mathbb{E} \\left[ \\boldsymbol{y} \\right] = \\boldsymbol{X \\beta}$, leading to:\n",
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}_{\\text{Ridge}}}) = \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{X \\beta}\n",
    "$$\n",
    "\n",
    "Which we were to show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6f914",
   "metadata": {},
   "source": [
    "**b)** Why do we say that Ridge regression gives a biased estimate? Is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8261f",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> We say that Ridge regression gives a biased estimate because the it's a systematic error from the true coefficients $\\boldsymbol{\\beta}$. This is  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e721fc",
   "metadata": {},
   "source": [
    "**c)** Show that the variance is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090eb1e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}]=\\sigma^2[  \\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}  \\mathbf{X}^{T}\\mathbf{X} \\{ [  \\mathbf{X}^{\\top} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}\\}^{T}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e8697",
   "metadata": {},
   "source": [
    "We see that if the parameter $\\lambda$ goes to infinity then the variance of the Ridge parameters $\\boldsymbol{\\beta}$ goes to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ed79b",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> Similar to what we did for the expectation value, we can use the variance of $\\boldsymbol{\\hat{\\beta}_{OLS}}$, and smuggle in the $\\lambda$ term, as it is just to be considered a non stochastic constant and thus won't effect any of the calculation leading up to the final expression. Meaning it just hides inside $A$ when we use the rule: $\\mathbf{Var}(\\boldsymbol{a} + \\boldsymbol{AM}) = \\boldsymbol{A} \\, \\mathbf{Var}(\\boldsymbol{M}) \\, \\boldsymbol{A}^T$. Eventually leading to:\n",
    "$$\n",
    "\n",
    "\\mathbf{Var}(\\hat{\\beta}_{\\text{Ridge}}) = \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\, \\sigma^2 \\, \\boldsymbol{I} \\, \\boldsymbol{X} \\left( \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Var}(\\hat{\\beta}_{\\text{Ridge}}) = \\sigma^2 \\, \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{X} \\left( \\left( \\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I} \\right)^{-1} \\right)^T\n",
    "$$\n",
    "\n",
    "As we were to show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc300b",
   "metadata": {},
   "source": [
    "## Exercise 3: Deriving the expression for the Bias-Variance Trade-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb86010",
   "metadata": {},
   "source": [
    "The aim of this exercise is to derive the equations for the bias-variance tradeoff to be used in project 1.\n",
    "\n",
    "The parameters $\\boldsymbol{\\boldsymbol{\\hat{\\beta}_{OLS}}}$ are found by optimizing the mean squared error via the so-called cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a0d1d",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) =\\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db06c",
   "metadata": {},
   "source": [
    "**a)** Show that you can rewrite this into an expression which contains\n",
    "\n",
    "- the variance of the model (the variance term)\n",
    "- the expected deviation of the mean of the model from the true data (the bias term)\n",
    "- the variance of the noise\n",
    "\n",
    "In other words, show that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\\mathrm{Bias}[\\tilde{y}]+\\mathrm{var}[\\tilde{y}]+\\sigma^2,\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mathrm{Bias}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\boldsymbol{y}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right],\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\tilde{\\boldsymbol{y}}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right]=\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d3af0",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> We start with: \n",
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) = \\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dafa9",
   "metadata": {},
   "source": [
    "We have previously made the assumption that $\\boldsymbol{y} = \\boldsymbol{f} + \\varepsilon$ which we subtitute intot he expression above. We also "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbfcd7",
   "metadata": {},
   "source": [
    "**b)** Explain what the terms mean and discuss their interpretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78dbc1a",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> The bias of the predicted $y$ values, $\\text{Bias}\\left[\\boldsymbol{\\tilde y}\\right]$ is the mean of the squared difference between prediction and true value. In other words the MSE of the prediction and true value. It is a metric over how much on average the prediction misses on the true value.\n",
    "The variance of the prediction, $\\tilde y$, is a measure how much the predictions varies from the expected prediction value based on the mean.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8b9d1",
   "metadata": {},
   "source": [
    "## Exercise 4: Computing the Bias and Variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e012430",
   "metadata": {},
   "source": [
    "Before you compute the bias and variance of a real model for different complexities, let's for now assume that you have sampled predictions and targets for a single model complexity using bootstrap resampling.\n",
    "\n",
    "**a)** Using the expression above, compute the mean squared error, bias and variance of the given data. Check that the sum of the bias and variance correctly gives (approximately) the mean squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b5bf581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n = 100\n",
    "bootstraps = 1000\n",
    "\n",
    "predictions = np.random.rand(bootstraps, n) * 10 + 10\n",
    "# targets = np.random.rand(bootstraps, n) # wrong\n",
    "targets = np.random.rand(1, n) # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "952ee45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.006949836719055\n",
      "15.006949836719057\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(predictions))\n",
    "print(np.mean(np.mean(predictions, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "602de673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for targets and predictions: 210.6763\n",
      "The bias: 210.6636\n",
      "The variance: 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Method of averaging over all samples first\n",
    "mean_samples_predictions = np.mean(predictions, axis=0) # mean of each prediction index over all samples\n",
    "targets = np.ravel(targets)\n",
    "\n",
    "mse = mean_squared_error(targets, mean_samples_predictions)\n",
    "bias = np.mean((targets - np.mean(mean_samples_predictions))**2)\n",
    "variance = np.mean((mean_samples_predictions - np.mean(mean_samples_predictions))**2)\n",
    "\n",
    "print(f\"The MSE for targets and predictions: {mse:.4f}\")\n",
    "print(f\"The bias: {bias:.4f}\")\n",
    "print(f\"The variance: {variance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "87d894d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for targets and predictions: 219.0463\n",
      "The bias: 210.6636\n",
      "The variance: 0.0081\n"
     ]
    }
   ],
   "source": [
    "# Method of calculating values for each sample before taking averages\n",
    "sq_error_pred_targ = (predictions - targets)**2 # squared errors of corresponding predictions and targets for each bootstrap sample\n",
    "mean_samples_sq_error = np.mean(sq_error_pred_targ, axis=0) # average of the error for each datapoint across all samples\n",
    "mse = np.sum(mean_samples_sq_error)/n\n",
    "\n",
    "y = np.ravel(targets)\n",
    "y_tilde = np.mean(predictions, axis=0)\n",
    "\n",
    "bias = np.mean((y - np.mean(y_tilde))**2)\n",
    "variance = np.mean((y_tilde - np.mean(y_tilde))**2)\n",
    "#variance = np.mean((predictions - np.mean(predictions))**2)\n",
    "\n",
    "print(f\"The MSE for targets and predictions: {mse:.4f}\")\n",
    "print(f\"The bias: {bias:.4f}\")\n",
    "print(f\"The variance: {variance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf92da",
   "metadata": {},
   "source": [
    "There is something very weird going on with my variance estimates and I can't figure it out.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e264999",
   "metadata": {},
   "source": [
    "**b)** Change the prediction values in some way to increase the bias while decreasing the variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c0147",
   "metadata": {},
   "source": [
    "\n",
    "**c)** Change the prediction values in some way to increase the variance while decreasing the bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da63362",
   "metadata": {},
   "source": [
    "**d)** Perform a bias-variance analysis of a polynomial OLS model fit to a one-dimensional function by computing and plotting the bias and variances values as a function of the polynomial degree of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "dd5855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import (\n",
    "    PolynomialFeatures,\n",
    ")  # use the fit_transform method of the created object!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7e35fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "bootstraps = 100\n",
    "              \n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-(x**2)) + 1.5*np.exp(-(x - 2)**2) + np.random.normal(0, 0.2, n)\n",
    "\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "max_degree = 14\n",
    "poly_degrees = np.arange(1, max_degree + 1, 1)\n",
    "\n",
    "biases = np.zeros(max_degree)\n",
    "variances = np.zeros(max_degree)\n",
    "MSEs = np.zeros(max_degree)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "for degree in range(len(poly_degrees)):\n",
    "    poly_features = PolynomialFeatures(poly_degrees[degree], include_bias=True)\n",
    "    X_train = poly_features.fit_transform(x_train)\n",
    "    X_test = poly_features.transform(x_test)\n",
    "    \n",
    "    # predictions are later filled with predictions made from X_test (feature matrix), constructed from x_test, \n",
    "    # so got to have same length\n",
    "    predictions = np.zeros([bootstraps, len(x_test)])\n",
    "\n",
    "    for b in range(bootstraps):\n",
    "        # For each bootstrap sample of X_train and Y_train, we train model, predict on X_test\n",
    "        # Later comparing against the un-touched y_test\n",
    "        #\n",
    "        X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "\n",
    "        model = LinearRegression().fit(X_train_resampled, y_train_resampled)\n",
    "        predictions[b,:] = model.predict(X_test).ravel()\n",
    "\n",
    "    # We take the true values or target, as the un-tough values in the y_test split\n",
    "    # The predicted y values, lives in the predictions matrix, where each row is a sample of values,\n",
    "    # and each column corresponding to a one y point across bootstrap samples \n",
    "    biases[degree] = np.mean((y_test - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "    # Var(prediction) is the mean of the flatend matrix, over all samples\n",
    "    variances[degree] = np.mean((predictions - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "    # For the MSE, we take difference of each y point per bootstrap sample, making y_test a row vector\n",
    "    # then squaring, before taking the mean over the flattened matrix\n",
    "\n",
    "    MSEs[degree] = np.mean(np.mean((predictions - y_test.T)**2, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5ff62dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(poly_degrees, biases, 'o-', label=\"Bias\")\n",
    "plt.plot(poly_degrees, variances, 'o-', label=\"Variance\")\n",
    "plt.plot(poly_degrees, MSEs, 'o-', label=\"MSE\")\n",
    "plt.title(f\"Bias-variance tradeoff, for {n} data points and {bootstraps} bootstraps\")\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b8461",
   "metadata": {},
   "source": [
    "**e)** Discuss the bias-variance trade-off as function of your model complexity (the degree of the polynomial).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb7087",
   "metadata": {},
   "source": [
    "<u>**Answer:**</u> There is something off here with the bias or MSE, but it seem to be correct in the code which corresponds to the expressions above. The bias + Var should be close to MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0864557",
   "metadata": {},
   "source": [
    "\n",
    "**f)** Compute and discuss the bias and variance as function of the number of data points (choose a suitable polynomial degree to show something interesting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d94ede17",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 28 (3118860759.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[282], line 30\u001b[1;36m\u001b[0m\n\u001b[1;33m    for b in range(bootstraps):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 28\n"
     ]
    }
   ],
   "source": [
    "poly_degree = 5\n",
    "\n",
    "n = 100\n",
    "ns = np.arange(10, n, 10)\n",
    "\n",
    "bootstraps = 100\n",
    "              \n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-(x**2)) + 1.5*np.exp(-(x - 2)**2) + np.random.normal(0, 0.2, n)\n",
    "\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "biases = np.zeros(max_degree)\n",
    "variances = np.zeros(max_degree)\n",
    "MSEs = np.zeros(max_degree)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "poly_features = PolynomialFeatures(poly_degrees[poly_degree], include_bias=True)\n",
    "X_train = poly_features.fit_transform(x_train)\n",
    "X_test = poly_features.transform(x_test)\n",
    "    \n",
    "# predictions are later filled with predictions made from X_test (feature matrix), constructed from x_test, \n",
    "# so got to have same length\n",
    "predictions = np.zeros([bootstraps, len(x_test)])\n",
    "\n",
    "for i in range(len(ns)):\n",
    "\n",
    "for b in range(bootstraps):\n",
    "    # For each bootstrap sample of X_train and Y_train, we train model, predict on X_test\n",
    "    # Later comparing against the un-touched y_test\n",
    "    X_train_resampled, y_train_resampled = resample(X_train, y_train)\n",
    "\n",
    "    model = LinearRegression().fit(X_train_resampled, y_train_resampled)\n",
    "    predictions[b,:] = model.predict(X_test).ravel()\n",
    "\n",
    "# We take the true values or target, as the un-tough values in the y_test split\n",
    "# The predicted y values, lives in the predictions matrix, where each row is a sample of values,\n",
    "# and each column corresponding to a one y point across bootstrap samples \n",
    "biases[degree] = np.mean((y_test - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "# Var(prediction) is the mean of the flatend matrix, over all samples\n",
    "variances[degree] = np.mean((predictions - np.mean(predictions, axis=0))**2)\n",
    "\n",
    "# For the MSE, we take difference of each y point per bootstrap sample, making y_test a row vector\n",
    "# then squaring, before taking the mean over the flattened matrix\n",
    "\n",
    "MSEs[degree] = np.mean(np.mean((predictions - y_test.T)**2, axis=0), axis=0)\n",
    "\n",
    "plt.plot(biases, 'o-', label=\"Bias\")\n",
    "plt.plot(variances, 'o-', label=\"Variance\")\n",
    "plt.plot(MSEs, 'o-', label=\"MSE\")\n",
    "plt.title(f\"Bias-variance tradeoff, for {n} data points and {bootstraps} bootstraps\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839e819",
   "metadata": {},
   "source": [
    "Work in progress.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46250fbc",
   "metadata": {},
   "source": [
    "## Exercise 5: Interpretation of scaling and metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af53055",
   "metadata": {},
   "source": [
    "In this course, we often ask you to scale data and compute various metrics. Although these practices are \"standard\" in the field, we will require you to demonstrate an understanding of _why_ you need to scale data and use these metrics. Both so that you can make better arguements about your results, and so that you will hopefully make fewer mistakes.\n",
    "\n",
    "First, a few reminders: In this course you should always scale the columns of the feature matrix, and sometimes scale the target data, when it is worth the effort. By scaling, we mean subtracting the mean and dividing by the standard deviation, though there are many other ways to scale data. When scaling either the feature matrix or the target data, the intercept becomes a bit harder to implement and understand, so take care.\n",
    "\n",
    "Briefly answer the following:\n",
    "\n",
    "**a)** Why do we scale data?\n",
    "\n",
    "**b)** Why does the OLS method give practically equivelent models on scaled and unscaled data?\n",
    "\n",
    "**c)** Why does the Ridge method **not** give practically equivelent models on scaled and unscaled data? Why do we only consider the model on scaled data correct?\n",
    "\n",
    "**d)** Why do we say that the Ridge method gives a biased model?\n",
    "\n",
    "**e)** Is the MSE of the OLS method affected by scaling of the feature matrix? Is it affected by scaling of the target data?\n",
    "\n",
    "**f)** Read about the R2 score, a metric we will ask you to use a lot later in the course. Is the R2 score of the OLS method affected by scaling of the feature matrix? Is it affected by scaling of the target data?\n",
    "\n",
    "**g)** Give interpretations of the following R2 scores: 0, 0.5, 1.\n",
    "\n",
    "**h)** What is an advantage of the R2 score over the MSE?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9a6f7",
   "metadata": {},
   "source": [
    "<u>Answers:</u>\n",
    "**a)** We scale data so that features with large numerical value won't override the other features. Also, the unit of the feature might play in on the fit, for instance pounds vs. kg."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
