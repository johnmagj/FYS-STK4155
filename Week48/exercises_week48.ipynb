{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2501d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5111",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef837a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ef66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear and logistic regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c9231",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "* There is a linearity between predictors/features and target/outout\n",
    "\n",
    " * The inputs/features distributed according to a normal/gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936c4e9",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> For linear regression and OLS we assumes that a target, $y$, can be approximated by $\\tilde y$ as a linear combination of the features, $\\boldsymbol{x}$, and some parameters, $\\boldsymbol{\\beta}$: \n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} \\boldsymbol{\\beta} = \\tilde y\n",
    "$$\n",
    "\n",
    "Then $y = \\tilde y + \\epsilon$, for some error $\\epsilon$.\n",
    "\n",
    "There are no constraint on the numerical value of the features or that they are sampled from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef906",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4711dda",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> This is true. Twice differentiating the MSE cost function with respect to the parameters, $\\boldsymbol{\\theta}$, (i. e the Hessian matrix) result in the transpose of the feature matrix, times it self, times a positive factor. The product is then a symmetric matrix with all positive elements which is always positive semi definite. This is enough to say that the cost function must be convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf02e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Which statement about logistic regression is false?\n",
    "\n",
    "* Logistic regression is used for binary classification.\n",
    "\n",
    " * It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    " * It has an analytical closed-form solution.\n",
    "\n",
    " * Its log-loss (cross-entropy) is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d27dcf",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> Logistic regression does not have an analytical closed-form solution. This is due to the cost function used, the binary cross entropy, which is not linear in the parameters (they appear as powers to the exponential function ) and we are not as fortunate as with OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab306a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "Logistic regression produces a linear decision boundary in the input space. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057219e",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695e6bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "Give two reasons why logistic regression is preferred over linear regression for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9c9aa",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> For binary classification we want to separate the data into two classes, \"yes\"/\"no\", 1/0. In logistic regression the sigmoid function gives a value between 0 and 1 which can be taught of as the probability of the input belonging to the \"yes\" or 1 class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fac35",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "* Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    " * Training relies on backpropagation using the chain rule.\n",
    "\n",
    " * A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    " * The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e6d5e",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> The loss surface of a deep neural network is not convex (with respect to the weights and biases). This is due to the nonlinear activation functions used at each layer like ReLU and sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed2727",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 7:\n",
    "\n",
    "Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7049f05",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> True. The sigmoid activation function return values between $0$ and $1$. The derivate of the sigmoid function resembles the shape of a somewhat wider normal distribution where the values at the peak is smaller than 1 and quickly approaches zero at the tails. With back propagation and the chain rule applied over many hidden layers, the derivatives are multiplied with each other as we move backwards layer by layer, and the small numerical value for each derivative can cause the gradient to approach zero. This will limit the training of the weights in the earlier layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1865d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 8:\n",
    "\n",
    "Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2bd9c",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> One way is to use activation functions that are not restricting the output and has a favorable derivative like the ReLU, which caps negative inputs at zero, work as a linear function for positive inputs, and has a derivative equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad1a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 9:\n",
    "\n",
    "Consider a fully-connected network with layer sizes $n_0$ (the input\n",
    "layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the\n",
    "output layer. Derive a general formula for the total number of\n",
    "trainable parameters (weights + biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68fec7",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> For a given node in a given layer there are one weight per incoming feature/activation (nodes) in the previous layer. Thus at a given node there are as many weights as nodes in the previous. This times as many nodes in the current layer. There is one bias to be trained per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a83",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 10:\n",
    "\n",
    "Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "* Local receptive fields\n",
    "\n",
    " * Weight sharing\n",
    "\n",
    " * More parameters than fully-connected layers\n",
    "\n",
    " * Pooling layers offering some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cda3e5",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefcc46",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 11:\n",
    "\n",
    "Using zero-padding in convolutional layers can preserve the input\n",
    "spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1,\n",
    "and padding $P = 1$. True or False?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e4874",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> Having a padding of $P = 1$ we frame the input with one row of zeros top and bottom, and one column of zeros on each side. With a stride size of 1, the center of the kernel will hit each of the pixels of the original input once, and since the kernel produce one output value through convolution for each stride the spatial dimension of the input is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6806",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 12:\n",
    "\n",
    "Given input width $W$, kernel size $K$, stride S, and padding P,\n",
    "derive the formula for the output width $W_{\\text{out}} = \\frac{W - K+ 2P}{S} + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a592ef",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629397f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 13:\n",
    "\n",
    "A convolutional layer has: $C_{\\text{in}}$ input channels,\n",
    "$C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times\n",
    "K_w$. Compute the number of trainable parameters including biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e6647",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087780b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5f95",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 14:\n",
    "\n",
    "Which statement about simple  RNNs is false?\n",
    "\n",
    "* They maintain a hidden state updated each time step.\n",
    "\n",
    " * They use the same weight matrices at every time step.\n",
    "\n",
    " * They handle sequences of arbitrary length.\n",
    "\n",
    " * They eliminate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf877b",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70bb6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 15:\n",
    "\n",
    "LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c141e",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec77a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 16:\n",
    "\n",
    "What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8076574",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u> BPTT is the process of of updating the weights and biases in an RNN. As the nodes in an RNN are interconnected layer wise, in a way that account for an sequential reliance in time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e01d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 17:\n",
    "\n",
    "What does a sliding window do? And why would we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e33a95",
   "metadata": {},
   "source": [
    "<u>**Answers:**</u>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
